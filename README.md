# crantconnectome-haberkernlab-meshcreation

Mesh creation workflow for the Haberkern Lab for clonal raider ant connectome (CRANTb).

## Overview

This is a simple CLI workflow that uses [Igneous](https://github.com/seung-lab/igneous) mesh generation tasks in the backend. The idea is to use 3D TIFF images of meshes (generated in Thermo Fisher AMIRA) to generate neuropil meshes that can be visualized in [Neuroglancer](https://github.com/google/neuroglancer).

## Installation

```bash
uv sync
```

## Usage

```bash
python tiff_to_mesh.py --d <directory of your 3d tiff file> \
                       --out <your output directory> \
                       --res <resolution of your mesh> \
                       --unsharded \
                       --setgit
```

**Flags:**

| Flag | Description | Default |
|------|-------------|---------|
| `--d` | Directory containing your 3D TIFF file (required) | - |
| `--out` | Output directory for meshes | Same as `--d` |
| `--res` | Voxel resolution in nm (three integers) | `8 8 42` |
| `--unsharded` | Use [unsharded](https://github.com/google/neuroglancer/blob/master/src/datasource/precomputed/meshes.md#unsharded-storage-of-multi-resolution-mesh-manifest) mesh format (default is [sharded](https://github.com/google/neuroglancer/blob/master/src/datasource/precomputed/meshes.md#sharded-storage-of-multi-resolution-mesh-manifest)) | Sharded |
| `--setgit` | Initialize a git repo in output for Neuroglancer | Disabled |

### Example

```bash
python tiff_to_mesh.py --d ./my_segmentation \
                       --out ./meshes \
                       --res 8 8 42 \
                       --setgit
```

## Adding the Mesh to Neuroglancer

After running with `--setgit`, push the generated mesh to GitHub. Then add it to your Neuroglancer state:

1. Click the **+** button to add a new source
2. Paste the raw GitHub content URL pointing to your mesh directory:

```
https://raw.githubusercontent.com/<username>/<repo>/<commit>/mesh/|neuroglancer-precomputed:
```

![Neuroglancer Layers](readme_images/neuroglancer_layers.png)

3. Configure the source with the correct resolution and transform:

![Neuroglancer Source Config](readme_images/neuroglancer_source.png)

## Aligning the Mesh

Initially, your mesh will not be aligned with the segmentation. Use these transform parameters:

| Parameter | Value |
|-----------|-------|
| Scale | `800 800 840` nm |
| Translation | `-5400 -5400 -60` (x, y, z) |

After applying this transform, the mesh will align with the segments and brain mesh for proofreading.

## Output Structure

The output follows the [Neuroglancer precomputed format](https://github.com/google/neuroglancer/blob/master/src/datasource/precomputed/README.md). The top-level `info` file describes the volume (data type, resolution, chunk layout), and the `mesh/` subdirectory contains multi-resolution Draco-compressed meshes generated by [Igneous](https://github.com/seung-lab/igneous).

```
output_volume/
├── info              # Precomputed volume metadata (JSON)
├── <scale_key>/      # Raw segmentation chunks
├── mesh/             # Generated meshes
│   ├── info          # Mesh metadata (JSON)
│   └── *.shard       # Sharded mesh files (or per-segment files if --unsharded)
└── .git/             # Git repo (if --setgit)
```
